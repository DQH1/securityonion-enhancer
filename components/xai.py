"""
Explainable AI (XAI) module for network anomaly detection system.
Contains functions for generating explanations from ML models using SHAP and Autoencoder analysis.
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Optional, Any

# Set up logging
logger = logging.getLogger(__name__)
# Silence verbose XAI logs by default (can be overridden by app configs)
logger.setLevel(logging.WARNING)
logger.propagate = True

# Human-readable interpretations for technical features
HUMAN_INTERPRETATIONS = {
    "Conn State S0": "[H√†nh vi ƒë√°ng ng·ªù] K·∫øt n·ªëi ƒë∆∞·ª£c g·ª≠i ƒëi nh∆∞ng **kh√¥ng nh·∫≠n l·∫°i b·∫•t k·ª≥ ph·∫£n h·ªìi n√†o (State: S0)**. ƒê√¢y l√† d·∫•u hi·ªáu kinh ƒëi·ªÉn c·ªßa k·ªπ thu·∫≠t qu√©t m·∫°ng (Network/Port Scanning).",
    "Conn State REJ": "[H√†nh vi ƒë√°ng ng·ªù] K·∫øt n·ªëi **b·ªã t·ª´ ch·ªëi th·∫≥ng th·ª´ng (State: REJ)**. Vi·ªác n√†y x·∫£y ra h√†ng lo·∫°t th∆∞·ªùng l√† d·∫•u hi·ªáu c·ªßa vi·ªác qu√©t c√°c port ƒë√£ ƒë√≥ng.",
    "Conn State RSTR": "[H√†nh vi ƒë√°ng ng·ªù] K·∫øt n·ªëi ƒë∆∞·ª£c thi·∫øt l·∫≠p r·ªìi **b·ªã ch√≠nh b√™n g·ª≠i reset ngay l·∫≠p t·ª©c (State: RSTR)**. Th∆∞·ªùng th·∫•y trong c√°c c√¥ng c·ª• brute-force ho·∫∑c qu√©t l·ªói ·ª©ng d·ª•ng.",
    "Conn State RSTO": "[H√†nh vi ƒë√°ng ng·ªù] K·∫øt n·ªëi **b·ªã ƒë√≠ch reset (State: RSTO)**, c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa vi·ªác t·ª´ ch·ªëi k·∫øt n·ªëi ho·∫∑c ph√≤ng th·ªß ch·ªëng t·∫•n c√¥ng.",
    "Service Binned Unknown": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] Giao d·ªãch ƒëang di·ªÖn ra qua m·ªôt **d·ªãch v·ª• kh√¥ng x√°c ƒë·ªãnh/kh√¥ng ph·ªï bi·∫øn**. K·∫ª t·∫•n c√¥ng th∆∞·ªùng s·ª≠ d·ª•ng c√°c port/d·ªãch v·ª• l·∫° ƒë·ªÉ l·∫©n tr√°nh s·ª± ph√°t hi·ªán.",
    "Duration": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **Th·ªùi gian k·∫øt n·ªëi c·ª±c ng·∫Øn**, ƒë√¢y l√† ƒë·∫∑c ƒëi·ªÉm c·ªßa c√°c k·∫øt n·ªëi thƒÉm d√≤, qu√©t l·ªói ho·∫∑c m·ªôt s·ªë lo·∫°i t·∫•n c√¥ng brute-force.",
    "Connection Duration": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **Th·ªùi gian k·∫øt n·ªëi b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ qu√° ng·∫Øn (scanning) ho·∫∑c qu√° d√†i (data exfiltration).",
    "Orig Ip Bytes": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **L∆∞·ª£ng d·ªØ li·ªáu g·ª≠i ƒëi c√≥ s·ª± b·∫•t th∆∞·ªùng** (qu√° l·ªõn ho·∫∑c qu√° nh·ªè), c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa vi·ªác tu·ªìn d·ªØ li·ªáu ho·∫∑c g·ª≠i c√°c g√≥i tin thƒÉm d√≤.",
    "Bytes Sent": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **L∆∞·ª£ng d·ªØ li·ªáu g·ª≠i ƒëi b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ ch·ªâ ra ho·∫°t ƒë·ªông data exfiltration ho·∫∑c command injection.",
    "Orig Port Binned Well-Known": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] K·∫øt n·ªëi xu·∫•t ph√°t t·ª´ m·ªôt **c·ªïng trong d·∫£i 'well-known' (0-1023)**. ƒê√¢y l√† h√†nh vi kh√¥ng b√¨nh th∆∞·ªùng ƒë·ªëi v·ªõi m·ªôt m√°y kh√°ch th√¥ng th∆∞·ªùng.",
    "Resp Ip Bytes": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **L∆∞·ª£ng d·ªØ li·ªáu ph·∫£n h·ªìi b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ ch·ªâ ra vi·ªác khai th√°c l·ªói ho·∫∑c thu th·∫≠p th√¥ng tin t·ª´ h·ªá th·ªëng m·ª•c ti√™u.",
    "Bytes Received": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **L∆∞·ª£ng d·ªØ li·ªáu nh·∫≠n v·ªÅ b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ l√† k·∫øt qu·∫£ c·ªßa data harvesting ho·∫∑c malware download.",
    "Resp Port Binned": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] **C·ªïng ƒë√≠ch c√≥ ƒë·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa vi·ªác qu√©t port ho·∫∑c t·∫•n c√¥ng v√†o c√°c d·ªãch v·ª• c·ª• th·ªÉ.",
    "Proto tcp": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] S·ª≠ d·ª•ng giao th·ª©c TCP trong ng·ªØ c·∫£nh b·∫•t th∆∞·ªùng, th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c cu·ªôc t·∫•n c√¥ng c√≥ m·ª•c ti√™u c·ª• th·ªÉ.",
    "Proto udp": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] S·ª≠ d·ª•ng giao th·ª©c UDP trong ng·ªØ c·∫£nh b·∫•t th∆∞·ªùng, c√≥ th·ªÉ li√™n quan ƒë·∫øn c√°c cu·ªôc t·∫•n c√¥ng DDoS ho·∫∑c qu√©t m·∫°ng nhanh.",
    "Conn State SF": "[H√†nh vi b√¨nh th∆∞·ªùng] K·∫øt n·ªëi ho√†n th√†nh b√¨nh th∆∞·ªùng (State: SF), nh∆∞ng c√≥ th·ªÉ c√≥ c√°c ƒë·∫∑c ƒëi·ªÉm kh√°c b·∫•t th∆∞·ªùng.",
    "Service Binned": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] D·ªãch v·ª• ƒë∆∞·ª£c ph√¢n lo·∫°i c√≥ ƒë·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng, c·∫ßn xem x√©t th√™m c√°c y·∫øu t·ªë kh√°c ƒë·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô nguy hi·ªÉm.",
    # Enhanced network behavior features
    "Bytes Ratio": "[H√†nh vi ƒë√°ng ng·ªù] **T·ª∑ l·ªá upload/download b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ ch·ªâ ra data exfiltration (upload cao) ho·∫∑c malware download (download cao).",
    "Packets Ratio": "[H√†nh vi ƒë√°ng ng·ªù] **T·ª∑ l·ªá g√≥i tin g·ª≠i/nh·∫≠n b·∫•t th∆∞·ªùng**, th∆∞·ªùng th·∫•y trong port scanning (nhi·ªÅu g√≥i g·ª≠i, √≠t ph·∫£n h·ªìi).",
    "Avg Packet Size Orig": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **K√≠ch th∆∞·ªõc g√≥i tin g·ª≠i ƒëi b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ ch·ªâ ra DNS tunneling (g√≥i nh·ªè) ho·∫∑c data exfiltration (g√≥i l·ªõn).",
    "Avg Packet Size Resp": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **K√≠ch th∆∞·ªõc g√≥i tin ph·∫£n h·ªìi b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa information gathering ho·∫∑c payload delivery.",
    "Connection Rate": "[H√†nh vi ƒë√°ng ng·ªù] **T·∫ßn su·∫•t k·∫øt n·ªëi b·∫•t th∆∞·ªùng**, t·∫ßn su·∫•t cao c√≥ th·ªÉ ch·ªâ ra C2 beaconing ho·∫∑c automated scanning.",
    "Failed Connection Ratio": "[H√†nh vi ƒë√°ng ng·ªù] **K·∫øt n·ªëi th·∫•t b·∫°i**, d·∫•u hi·ªáu r√µ r√†ng c·ªßa port scanning, brute force attacks, ho·∫∑c reconnaissance activities.",
    "Packets Sent": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **S·ªë l∆∞·ª£ng g√≥i tin g·ª≠i ƒëi b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ ch·ªâ ra flooding attacks ho·∫∑c scanning activities.",
    "Packets Received": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **S·ªë l∆∞·ª£ng g√≥i tin nh·∫≠n v·ªÅ b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ li√™n quan ƒë·∫øn data harvesting ho·∫∑c response analysis.",
    "Missed Bytes": "[ƒê·∫∑c ƒëi·ªÉm ƒë√°ng ng·ªù] **D·ªØ li·ªáu b·ªã m·∫•t trong qu√° tr√¨nh truy·ªÅn**, c√≥ th·ªÉ ch·ªâ ra network evasion techniques ho·∫∑c fragmentation attacks.",
    # Enhanced categorical features
    "Duration Category": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **Ph√¢n lo·∫°i th·ªùi gian k·∫øt n·ªëi b·∫•t th∆∞·ªùng**, c√°c k·∫øt n·ªëi qu√° ng·∫Øn ho·∫∑c qu√° d√†i so v·ªõi d·ªãch v·ª• th√¥ng th∆∞·ªùng.",
    "Traffic Pattern Upload Heavy": "[H√†nh vi ƒë√°ng ng·ªù] **M·∫´u traffic upload n·∫∑ng**, d·∫•u hi·ªáu m·∫°nh m·∫Ω c·ªßa data exfiltration ho·∫∑c command injection.",
    "Traffic Pattern Download Heavy": "[H√†nh vi ƒë√°ng ng·ªù] **M·∫´u traffic download n·∫∑ng**, c√≥ th·ªÉ ch·ªâ ra malware download ho·∫∑c data harvesting.",
    "Traffic Pattern Balanced": "[M·∫´u h√¨nh b√¨nh th∆∞·ªùng] **M·∫´u traffic c√¢n b·∫±ng**, nh∆∞ng c√≥ th·ªÉ che gi·∫•u c√°c ho·∫°t ƒë·ªông ƒë√°ng ng·ªù kh√°c.",
    "Traffic Pattern Burst": "[H√†nh vi ƒë√°ng ng·ªù] **M·∫´u traffic burst**, c√≥ th·ªÉ ch·ªâ ra scanning activities ho·∫∑c automated attacks.",
    "Traffic Pattern Large Transfer": "[H√†nh vi ƒë√°ng ng·ªù] **Truy·ªÅn t·∫£i d·ªØ li·ªáu l·ªõn**, c√≥ th·ªÉ l√† data exfiltration ho·∫∑c malware distribution.",
    "Traffic Pattern Single Packet": "[H√†nh vi ƒë√°ng ng·ªù] **Giao ti·∫øp m·ªôt g√≥i tin**, th∆∞·ªùng th·∫•y trong reconnaissance probes ho·∫∑c heartbeat checks.",
    # NEW: C2 Beaconing specific features
    "Small Consistent Size": "[H√†nh vi ƒë√°ng ng·ªù C2] **K√≠ch th∆∞·ªõc d·ªØ li·ªáu nh·ªè v√† nh·∫•t qu√°n**, ƒë√¢y l√† ƒë·∫∑c ƒëi·ªÉm ƒëi·ªÉn h√¨nh c·ªßa C2 beaconing - malware giao ti·∫øp v·ªõi command server b·∫±ng c√°c g√≥i tin nh·ªè, ƒë·ªÅu ƒë·∫∑n.",
    "Heartbeat Candidate": "[H√†nh vi ƒë√°ng ng·ªù C2] **K·∫øt n·ªëi heartbeat ƒëi·ªÉn h√¨nh**, c√≥ ƒë·∫∑c ƒëi·ªÉm c·ªßa C2 beaconing: th·ªùi gian ng·∫Øn, d·ªØ li·ªáu √≠t, k·∫øt n·ªëi th√†nh c√¥ng - d·∫•u hi·ªáu c·ªßa malware check-in v·ªõi C2 server.",
    "Periodic Score": "[H√†nh vi ƒë√°ng ng·ªù C2] **ƒêi·ªÉm s·ªë chu k·ª≥ cao**, ch·ªâ ra t√≠nh ƒë·ªÅu ƒë·∫∑n v·ªÅ th·ªùi gian - ƒë·∫∑c tr∆∞ng m·∫°nh c·ªßa automated C2 beaconing thay v√¨ human browsing behavior.",
    "Beaconing Pattern High Beacon Candidate": "[H√†nh vi ƒë√°ng ng·ªù C2] **M·∫´u h√¨nh C2 beaconing ƒë·ªô tin c·∫≠y cao**, k·∫øt n·ªëi c√≥ t·∫•t c·∫£ ƒë·∫∑c ƒëi·ªÉm c·ªßa C2 communication: nh·ªè, nhanh, th√†nh c√¥ng, ƒë·ªÅu ƒë·∫∑n.",
    "Beaconing Pattern Medium Beacon Candidate": "[H√†nh vi ƒë√°ng ng·ªù C2] **M·∫´u h√¨nh C2 beaconing ƒë·ªô tin c·∫≠y trung b√¨nh**, c√≥ m·ªôt s·ªë ƒë·∫∑c ƒëi·ªÉm c·ªßa C2 communication nh∆∞ng ch∆∞a r√µ r√†ng ho√†n to√†n.",
    "Beaconing Pattern Quick Probe": "[H√†nh vi ƒë√°ng ng·ªù C2] **M·∫´u h√¨nh thƒÉm d√≥ nhanh**, c√≥ th·ªÉ l√† C2 beaconing ho·∫∑c reconnaissance activity v·ªõi ƒë·∫∑c ƒëi·ªÉm k·∫øt n·ªëi r·∫•t ng·∫Øn v√† d·ªØ li·ªáu √≠t.",

    "Beaconing Pattern Minimal Exchange": "[H√†nh vi ƒë√°ng ng·ªù C2] **M·∫´u h√¨nh trao ƒë·ªïi t·ªëi thi·ªÉu**, ƒë·∫∑c ƒëi·ªÉm c·ªßa lightweight C2 communication ho·∫∑c heartbeat mechanism.",
    # Group Features from GroupFeatureTransformer - C2 Beaconing Detection
    "Beacon Group Count": "[H√†nh vi ƒë√°ng ng·ªù C2] **S·ªë l∆∞·ª£ng k·∫øt n·ªëi trong group beaconing**, ch·ªâ ra m·∫≠t ƒë·ªô communication ƒë·ªÅu ƒë·∫∑n - ƒë·∫∑c tr∆∞ng m·∫°nh c·ªßa C2 beaconing thay v√¨ human browsing.",
    "Beacon Group Mean Interval": "[H√†nh vi ƒë√°ng ng·ªù C2] **Kho·∫£ng th·ªùi gian trung b√¨nh gi·ªØa c√°c k·∫øt n·ªëi**, t√≠nh ƒë·ªÅu ƒë·∫∑n cao ch·ªâ ra automated C2 communication thay v√¨ random human behavior.",
    "Beacon Group Cv": "[H√†nh vi ƒë√°ng ng·ªù C2] **H·ªá s·ªë bi·∫øn thi√™n th·ªùi gian beaconing**, gi√° tr·ªã th·∫•p ch·ªâ ra t√≠nh nh·∫•t qu√°n cao - ƒë·∫∑c tr∆∞ng ƒëi·ªÉn h√¨nh c·ªßa automated C2 heartbeat.",
    
    #  FIX: Add missing Group Features for SHAP explanations
    "beacon_group_count": "[H√†nh vi ƒë√°ng ng·ªù C2] **S·ªë l∆∞·ª£ng k·∫øt n·ªëi beaconing trong nh√≥m**, ch·ªâ ra m·∫≠t ƒë·ªô communication ƒë·ªÅu ƒë·∫∑n - ƒë·∫∑c tr∆∞ng m·∫°nh c·ªßa C2 beaconing thay v√¨ human browsing.",
    "beacon_group_cv": "[H√†nh vi ƒë√°ng ng·ªù C2] **H·ªá s·ªë bi·∫øn thi√™n th·ªùi gian beaconing**, gi√° tr·ªã th·∫•p ch·ªâ ra t√≠nh nh·∫•t qu√°n cao - ƒë·∫∑c tr∆∞ng ƒëi·ªÉn h√¨nh c·ªßa automated C2 heartbeat.",
    "beacon_channel_timediff_std": "[H√†nh vi ƒë√°ng ng·ªù C2] **ƒê·ªô l·ªách chu·∫©n th·ªùi gian gi·ªØa c√°c k·∫øt n·ªëi beaconing**, ch·ªâ ra t√≠nh ƒë·ªÅu ƒë·∫∑n c·ªßa C2 communication.",
    "beacon_channel_duration_std": "[H√†nh vi ƒë√°ng ng·ªù C2] **ƒê·ªô l·ªách chu·∫©n th·ªùi gian k·∫øt n·ªëi beaconing**, ch·ªâ ra t√≠nh nh·∫•t qu√°n c·ªßa C2 sessions.",
    "beacon_channel_orig_bytes_std": "[H√†nh vi ƒë√°ng ng·ªù C2] **ƒê·ªô l·ªách chu·∫©n d·ªØ li·ªáu g·ª≠i ƒëi trong beaconing**, ch·ªâ ra t√≠nh ƒë·ªÅu ƒë·∫∑n c·ªßa C2 payload.",
    "horizontal_scan_unique_dst_ip_count": "[H√†nh vi ƒë√°ng ng·ªù] **S·ªë l∆∞·ª£ng IP ƒë√≠ch kh√°c nhau trong horizontal scanning**, ch·ªâ ra vi·ªác qu√©t nhi·ªÅu host kh√°c nhau t·ª´ c√πng m·ªôt port - d·∫•u hi·ªáu r√µ r√†ng c·ªßa network reconnaissance.",
    "horizontal_scan_problematic_ratio": "[H√†nh vi ƒë√°ng ng·ªù] **T·ª∑ l·ªá k·∫øt n·ªëi c√≥ v·∫•n ƒë·ªÅ trong horizontal scanning**, ch·ªâ ra m·ª©c ƒë·ªô th√†nh c√¥ng c·ªßa vi·ªác qu√©t m·∫°ng - t·ª∑ l·ªá cao c√≥ th·ªÉ ch·ªâ ra network mapping.",
    "vertical_scan_unique_dst_port_count": "[H√†nh vi ƒë√°ng ng·ªù] **S·ªë l∆∞·ª£ng port ƒë√≠ch kh√°c nhau trong vertical scanning**, ch·ªâ ra vi·ªác qu√©t nhi·ªÅu port kh√°c nhau t·ª´ c√πng m·ªôt host - d·∫•u hi·ªáu c·ªßa service enumeration.",
    "vertical_scan_problematic_ratio": "[H√†nh vi ƒë√°ng ng·ªù] **T·ª∑ l·ªá k·∫øt n·ªëi c√≥ v·∫•n ƒë·ªÅ trong vertical scanning**, ch·ªâ ra m·ª©c ƒë·ªô th√†nh c√¥ng c·ªßa vi·ªác qu√©t port - t·ª∑ l·ªá cao c√≥ th·ªÉ ch·ªâ ra service discovery.",
    "ddos_group_unique_src_ip_count": "[H√†nh vi ƒë√°ng ng·ªù DDoS] **S·ªë l∆∞·ª£ng IP ngu·ªìn kh√°c nhau trong nh√≥m DDoS**, ch·ªâ ra m·ª©c ƒë·ªô ph√¢n t√°n c·ªßa cu·ªôc t·∫•n c√¥ng - s·ªë l∆∞·ª£ng cao c√≥ th·ªÉ ch·ªâ ra botnet ho·∫∑c coordinated attack.",
    
    # Missing mappings for important SHAP features
    "Is Auth Port": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] **C·ªïng ƒë√≠ch c√≥ ƒë·∫∑c ƒëi·ªÉm authentication**, c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa vi·ªác t·∫•n c√¥ng v√†o c√°c d·ªãch v·ª• x√°c th·ª±c.",
    "Hist R Count": "[H√†nh vi ƒë√°ng ng·ªù] **S·ªë l∆∞·ª£ng g√≥i tin b·ªã reset trong l·ªãch s·ª≠ k·∫øt n·ªëi**, c√≥ th·ªÉ ch·ªâ ra scanning behavior ho·∫∑c connection probing.",
    "Is Auth Service": "[T·∫•n c√¥ng X√°c th·ª±c] H√†nh vi nh·∫Øm v√†o m·ªôt d·ªãch v·ª• x√°c th·ª±c (SSH, FTP...), m·ªôt m·ª•c ti√™u ph·ªï bi·∫øn c·ªßa t·∫•n c√¥ng brute-force.",
    "Failed Connection Ratio": "[H√†nh vi ƒë√°ng ng·ªù] **K·∫øt n·ªëi th·∫•t b·∫°i**, d·∫•u hi·ªáu r√µ r√†ng c·ªßa port scanning, brute force attacks, ho·∫∑c reconnaissance activities.",
    
    # DNS Log Features - B·ªï sung th√™m
    "Query Length": "[ƒê·ªô d√†i query DNS] **Query DNS qu√° d√†i** c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data exfiltration qua DNS, ho·∫∑c malicious payload encoding.",
    "Query Entropy": "[Entropy c·ªßa query DNS] **Entropy cao trong query DNS** c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c obfuscated communication.",
    "Subdomain Count": "[S·ªë l∆∞·ª£ng subdomain] **S·ªë subdomain b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data encoding, ho·∫∑c malicious domain generation.",
    "Numeric Ratio": "[T·ª∑ l·ªá k√Ω t·ª± s·ªë] **T·ª∑ l·ªá s·ªë trong query DNS cao** c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c binary data encoding.",
    "Ngram Score": "[ƒêi·ªÉm n-gram] **ƒêi·ªÉm n-gram b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra encoded data, obfuscated communication, ho·∫∑c malicious payload.",
    "Has Base64 Pattern": "[C√≥ pattern Base64] **Query DNS ch·ª©a Base64** c√≥ th·ªÉ ch·ªâ ra data encoding, DNS tunneling, ho·∫∑c malicious payload transmission.",
    "Has Hex Pattern": "[C√≥ pattern Hex] **Query DNS ch·ª©a Hex** c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c binary data transmission.",
    "Has Long Subdomain": "[C√≥ subdomain d√†i] **Subdomain qu√° d√†i** c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data encoding, ho·∫∑c malicious domain generation.",
    "Suspicious Length": "[ƒê·ªô d√†i ƒë√°ng ng·ªù] **Query DNS c√≥ ƒë·ªô d√†i ƒë√°ng ng·ªù** c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data exfiltration, ho·∫∑c malicious communication.",
    "Char Diversity": "[ƒêa d·∫°ng k√Ω t·ª±] **ƒêa d·∫°ng k√Ω t·ª± b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c obfuscated communication.",
    "Vowel Consonant Ratio": "[T·ª∑ l·ªá nguy√™n √¢m ph·ª• √¢m] **T·ª∑ l·ªá nguy√™n √¢m/ph·ª• √¢m b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c non-human generated queries.",
    "Compressed Pattern": "[Pattern n√©n] **Query DNS c√≥ pattern n√©n** c√≥ th·ªÉ ch·ªâ ra compressed data, DNS tunneling, ho·∫∑c obfuscated communication.",
    "Unusual TLD": "[TLD b·∫•t th∆∞·ªùng] **Top-level domain b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra malicious domain, DNS tunneling, ho·∫∑c command & control communication.",
    "Avg TTL": "[TTL trung b√¨nh] **TTL b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra DNS tunneling, malicious DNS, ho·∫∑c command & control infrastructure.",
    "Min TTL": "[TTL t·ªëi thi·ªÉu] **TTL t·ªëi thi·ªÉu b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra DNS tunneling, fast-flux DNS, ho·∫∑c malicious infrastructure.",
    "Is Qtype TXT": "[Query type TXT] **Query TXT b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data exfiltration, ho·∫∑c command & control communication.",
    "Is Qtype NULL": "[Query type NULL] **Query NULL b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data exfiltration, ho·∫∑c malicious DNS usage.",
    "Is NXDOMAIN": "[Response NXDOMAIN] **Response NXDOMAIN b·∫•t th∆∞·ªùng** c√≥ th·ªÉ ch·ªâ ra DNS tunneling, malicious domain queries, ho·∫∑c reconnaissance."
}

# Human-readable interpretations for Autoencoder features
# These explain why the model had difficulty reconstructing certain features
HUMAN_INTERPRETATIONS_AE = {
    "History Length": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] Chu·ªói s·ª± ki·ªán c·ªßa k·∫øt n·ªëi (connection history) c√≥ **ƒë·ªô d√†i ho·∫∑c c·∫•u tr√∫c r·∫•t l·∫°**, kh√¥ng gi·ªëng v·ªõi c√°c k·∫øt n·ªëi b√¨nh th∆∞·ªùng m√† model ƒë√£ h·ªçc.",
    "Timeout Flag": "[H√†nh vi ƒë√°ng ng·ªù] K·∫øt n·ªëi n√†y c√≥ **d·∫•u hi·ªáu b·ªã timeout (h·∫øt th·ªùi gian ch·ªù)**, m·ªôt ƒë·∫∑c ƒëi·ªÉm kh√¥ng th∆∞·ªùng th·∫•y trong c√°c giao d·ªãch th√†nh c√¥ng v√† c√≥ th·ªÉ ch·ªâ ra l·ªói m·∫°ng ho·∫∑c h√†nh vi thƒÉm d√≤.",
    "Service Binned Other": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] Giao d·ªãch ƒëang s·ª≠ d·ª•ng m·ªôt **d·ªãch v·ª• kh√¥ng ph·ªï bi·∫øn ho·∫∑c ƒë√£ b·ªã che gi·∫•u**, khi·∫øn model kh√≥ c√≥ th·ªÉ t√°i t·∫°o l·∫°i m·ªôt c√°ch ch√≠nh x√°c.",
    "Service Binned Unknown": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] Giao d·ªãch ƒëang s·ª≠ d·ª•ng m·ªôt **d·ªãch v·ª• kh√¥ng x√°c ƒë·ªãnh**, khi·∫øn model kh√≥ c√≥ th·ªÉ t√°i t·∫°o l·∫°i m·ªôt c√°ch ch√≠nh x√°c.",
    "Duration": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **Th·ªùi gian k·∫øt n·ªëi qu√° d√†i ho·∫∑c qu√° ng·∫Øn** so v·ªõi c√°c k·∫øt n·ªëi th√¥ng th∆∞·ªùng cho c√πng lo·∫°i d·ªãch v·ª•.",
    "Connection Duration": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **Th·ªùi gian k·∫øt n·ªëi b·∫•t th∆∞·ªùng**, model kh√¥ng th·ªÉ t√°i t·∫°o ch√≠nh x√°c do kh√°c bi·ªát so v·ªõi c√°c m·∫´u h·ªçc ƒë∆∞·ª£c.",
    "Orig Ip Bytes": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **L∆∞·ª£ng d·ªØ li·ªáu g·ª≠i ƒëi kh√¥ng nh·∫•t qu√°n** v·ªõi c√°c m·∫´u traffic th√¥ng th∆∞·ªùng, c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa vi·ªác tu·ªìn d·ªØ li·ªáu ho·∫∑c c√°c g√≥i tin C&C.",
    "Bytes Sent": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **L∆∞·ª£ng d·ªØ li·ªáu g·ª≠i ƒëi kh√°c th∆∞·ªùng**, model kh√≥ t√°i t·∫°o do kh√¥ng ph√π h·ª£p v·ªõi c√°c m·∫´u benign ƒë√£ h·ªçc.",
    "Resp Ip Bytes": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **L∆∞·ª£ng d·ªØ li·ªáu nh·∫≠n v·ªÅ c√≥ s·ª± sai kh√°c l·ªõn**, c√≥ th·ªÉ l√† k·∫øt qu·∫£ c·ªßa m·ªôt l·ªánh t·∫•n c√¥ng ho·∫∑c m·ªôt ph·∫£n h·ªìi l·ªói t·ª´ server.",
    "Bytes Received": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **L∆∞·ª£ng d·ªØ li·ªáu nh·∫≠n v·ªÅ b·∫•t th∆∞·ªùng**, model kh√¥ng th·ªÉ t√°i t·∫°o ch√≠nh x√°c do kh√°c bi·ªát v·ªõi traffic patterns th√¥ng th∆∞·ªùng.",
    "Conn State": "[Tr·∫°ng th√°i b·∫•t th∆∞·ªùng] **Tr·∫°ng th√°i k·∫øt th√∫c c·ªßa k·∫øt n·ªëi** (v√≠ d·ª•: S0, REJ) l√† m·ªôt y·∫øu t·ªë b·∫•t th∆∞·ªùng m·∫°nh m·∫Ω m√† model kh√¥ng mong ƒë·ª£i.",
    "Orig Port Binned": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] **C·ªïng ngu·ªìn c√≥ ƒë·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng**, kh√¥ng ph√π h·ª£p v·ªõi c√°c m·∫´u k·∫øt n·ªëi th√¥ng th∆∞·ªùng m√† model ƒë√£ h·ªçc.",
    "Resp Port Binned": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] **C·ªïng ƒë√≠ch c√≥ ƒë·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng**, c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa vi·ªác qu√©t port ho·∫∑c t·∫•n c√¥ng v√†o c√°c d·ªãch v·ª• c·ª• th·ªÉ.",
    "Proto": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **Giao th·ª©c m·∫°ng ƒë∆∞·ª£c s·ª≠ d·ª•ng** trong ng·ªØ c·∫£nh b·∫•t th∆∞·ªùng, kh√¥ng ph√π h·ª£p v·ªõi c√°c m·∫´u traffic th√¥ng th∆∞·ªùng.",
    "Service": "[Ng·ªØ c·∫£nh ƒë√°ng ng·ªù] **D·ªãch v·ª• m·∫°ng** c√≥ ƒë·∫∑c ƒëi·ªÉm kh√¥ng th∆∞·ªùng th·∫•y, khi·∫øn model kh√≥ t√°i t·∫°o ch√≠nh x√°c.",
    "Local Orig": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **ƒê·∫∑c t√≠nh ƒë·ªãa ph∆∞∆°ng c·ªßa ngu·ªìn k·∫øt n·ªëi** c√≥ s·ª± kh√°c bi·ªát so v·ªõi c√°c m·∫´u th√¥ng th∆∞·ªùng.",
    "Local Resp": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **ƒê·∫∑c t√≠nh ƒë·ªãa ph∆∞∆°ng c·ªßa ƒë√≠ch k·∫øt n·ªëi** c√≥ s·ª± kh√°c bi·ªát so v·ªõi c√°c m·∫´u th√¥ng th∆∞·ªùng.",
    # Enhanced network behavior features for AE
    "Bytes Ratio": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **T·ª∑ l·ªá upload/download** c√≥ s·ª± kh√°c bi·ªát l·ªõn so v·ªõi c√°c k·∫øt n·ªëi b√¨nh th∆∞·ªùng, model kh√≥ t√°i t·∫°o ch√≠nh x√°c.",
    "Packets Ratio": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **T·ª∑ l·ªá g√≥i tin g·ª≠i/nh·∫≠n** kh√¥ng ph√π h·ª£p v·ªõi c√°c m·∫´u traffic th√¥ng th∆∞·ªùng m√† model ƒë√£ h·ªçc.",
    "Avg Packet Size Orig": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **K√≠ch th∆∞·ªõc g√≥i tin g·ª≠i ƒëi** c√≥ s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ so v·ªõi c√°c k·∫øt n·ªëi benign, khi·∫øn model kh√≥ reconstruction.",
    "Avg Packet Size Resp": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **K√≠ch th∆∞·ªõc g√≥i tin ph·∫£n h·ªìi** kh√¥ng nh·∫•t qu√°n v·ªõi c√°c m·∫´u h·ªçc ƒë∆∞·ª£c, ch·ªâ ra h√†nh vi b·∫•t th∆∞·ªùng.",
    "Connection Rate": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **T·∫ßn su·∫•t k·∫øt n·ªëi** kh√°c bi·ªát so v·ªõi c√°c m·∫´u th√¥ng th∆∞·ªùng, c√≥ th·ªÉ ch·ªâ ra automated behavior.",
    "Failed Connection Ratio": "[H√†nh vi b·∫•t th∆∞·ªùng] **T·ª∑ l·ªá k·∫øt n·ªëi th·∫•t b·∫°i** l√† m·ªôt ƒë·∫∑c ƒëi·ªÉm m·∫°nh m·∫Ω kh√¥ng th∆∞·ªùng th·∫•y trong traffic b√¨nh th∆∞·ªùng.",
    "Packets Sent": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **S·ªë l∆∞·ª£ng g√≥i tin g·ª≠i ƒëi** kh√°c bi·ªát ƒë√°ng k·ªÉ so v·ªõi c√°c m·∫´u benign m√† model ƒë√£ h·ªçc.",
    "Packets Received": "[ƒê·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng] **S·ªë l∆∞·ª£ng g√≥i tin nh·∫≠n v·ªÅ** kh√¥ng ph√π h·ª£p v·ªõi c√°c pattern th√¥ng th∆∞·ªùng, khi·∫øn model kh√≥ t√°i t·∫°o.",
    "Missed Bytes": "[ƒê·∫∑c ƒëi·ªÉm ƒë√°ng ng·ªù] **D·ªØ li·ªáu b·ªã m·∫•t** l√† m·ªôt anomaly m·∫°nh m·∫Ω kh√¥ng th∆∞·ªùng xu·∫•t hi·ªán trong traffic b√¨nh th∆∞·ªùng.",
    # Enhanced categorical features for AE
    "Duration Category": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **Ph√¢n lo·∫°i th·ªùi gian k·∫øt n·ªëi** kh√¥ng ph√π h·ª£p v·ªõi c√°c m·∫´u ƒë√£ h·ªçc, ch·ªâ ra h√†nh vi b·∫•t th∆∞·ªùng.",
    "Traffic Pattern Upload Heavy": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **M·∫´u traffic upload n·∫∑ng** l√† ƒë·∫∑c ƒëi·ªÉm kh√¥ng th∆∞·ªùng th·∫•y trong benign traffic.",
    "Traffic Pattern Download Heavy": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **M·∫´u traffic download n·∫∑ng** kh√°c bi·ªát so v·ªõi c√°c m·∫´u th√¥ng th∆∞·ªùng m√† model ƒë√£ h·ªçc.",
    "Traffic Pattern Balanced": "[M·∫´u h√¨nh ƒë·∫∑c bi·ªát] **M·∫´u traffic c√¢n b·∫±ng** nh∆∞ng c√≥ c√°c ƒë·∫∑c ƒëi·ªÉm kh√°c khi·∫øn model kh√≥ t√°i t·∫°o ch√≠nh x√°c.",
    "Traffic Pattern Burst": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **M·∫´u traffic burst** l√† ƒë·∫∑c ƒëi·ªÉm kh√¥ng ph·ªï bi·∫øn trong c√°c k·∫øt n·ªëi b√¨nh th∆∞·ªùng.",
    "Traffic Pattern Large Transfer": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **M·∫´u truy·ªÅn t·∫£i d·ªØ li·ªáu l·ªõn** kh√°c bi·ªát ƒë√°ng k·ªÉ so v·ªõi traffic patterns th√¥ng th∆∞·ªùng.",
    "Traffic Pattern Single Packet": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng] **M·∫´u giao ti·∫øp m·ªôt g√≥i tin** l√† ƒë·∫∑c ƒëi·ªÉm kh√¥ng th∆∞·ªùng th·∫•y trong c√°c k·∫øt n·ªëi application-layer b√¨nh th∆∞·ªùng.",
    # NEW: C2 Beaconing specific features for AE
    "Small Consistent Size": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng C2] **K√≠ch th∆∞·ªõc d·ªØ li·ªáu nh·ªè v√† nh·∫•t qu√°n** l√† ƒë·∫∑c ƒëi·ªÉm kh√¥ng ph·ªï bi·∫øn trong benign traffic, model kh√≥ t√°i t·∫°o do t√≠nh ƒë·∫∑c tr∆∞ng c·ªßa C2 beaconing.",
    "Heartbeat Candidate": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng C2] **Pattern heartbeat ƒëi·ªÉn h√¨nh** kh√¥ng th∆∞·ªùng th·∫•y trong traffic b√¨nh th∆∞·ªùng, ch·ªâ ra kh·∫£ nƒÉng cao l√† automated C2 communication.",
    "Periodic Score": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng C2] **T√≠nh chu k·ª≥ cao** l√† ƒë·∫∑c ƒëi·ªÉm m·∫°nh m·∫Ω c·ªßa automated behavior, kh√°c bi·ªát ho√†n to√†n so v·ªõi human browsing patterns m√† model ƒë√£ h·ªçc.",
    "Beaconing Pattern High Beacon Candidate": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng C2] **Combination pattern c√≥ ƒë·ªô tin c·∫≠y cao** cho C2 beaconing, model kh√¥ng th·ªÉ t√°i t·∫°o do ch∆∞a t·ª´ng h·ªçc ƒë∆∞·ª£c pattern n√†y trong benign data.",
    "Beaconing Pattern Medium Beacon Candidate": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng C2] **Pattern c√≥ ƒë·∫∑c ƒëi·ªÉm C2 trung b√¨nh**, model g·∫∑p kh√≥ khƒÉn trong reconstruction do s·ª± kh√°c bi·ªát v·ªõi normal traffic patterns.",
    "Beaconing Pattern Quick Probe": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng C2] **Pattern thƒÉm d√≤ nhanh**, ƒë·∫∑c ƒëi·ªÉm kh√¥ng th∆∞·ªùng th·∫•y trong benign traffic, khi·∫øn model kh√≥ t√°i t·∫°o ch√≠nh x√°c.",
    "Beaconing Pattern Minimal Exchange": "[M·∫´u h√¨nh b·∫•t th∆∞·ªùng C2] **Pattern trao ƒë·ªïi t·ªëi thi·ªÉu**, l√† ƒë·∫∑c tr∆∞ng c·ªßa lightweight malware communication, kh√°c bi·ªát so v·ªõi legitimate application traffic.",
    # Group Features from GroupFeatureTransformer - C2 Beaconing Detection
    "Beacon Group Count": "[H√†nh vi ƒë√°ng ng·ªù C2] **S·ªë l∆∞·ª£ng k·∫øt n·ªëi trong group beaconing**, ch·ªâ ra m·∫≠t ƒë·ªô communication ƒë·ªÅu ƒë·∫∑n - ƒë·∫∑c tr∆∞ng m·∫°nh c·ªßa C2 beaconing thay v√¨ human browsing.",
    "Beacon Group Mean Interval": "[H√†nh vi ƒë√°ng ng·ªù C2] **Kho·∫£ng th·ªùi gian trung b√¨nh gi·ªØa c√°c k·∫øt n·ªëi**, t√≠nh ƒë·ªÅu ƒë·∫∑n cao ch·ªâ ra automated C2 communication thay v√¨ random human behavior.",
    "Beacon Group Std Interval": "[H√†nh vi ƒë√°ng ng·ªù C2] **ƒê·ªô l·ªách chu·∫©n th·ªùi gian gi·ªØa k·∫øt n·ªëi**, ƒë·ªô l·ªách th·∫•p ch·ªâ ra t√≠nh chu k·ª≥ ƒë·ªÅu ƒë·∫∑n c·ªßa malware beaconing.",
    "Beacon Group Cv": "[H√†nh vi ƒë√°ng ng·ªù C2] **H·ªá s·ªë bi·∫øn thi√™n th·ªùi gian beaconing**, gi√° tr·ªã th·∫•p ch·ªâ ra t√≠nh nh·∫•t qu√°n cao - ƒë·∫∑c tr∆∞ng ƒëi·ªÉn h√¨nh c·ªßa automated C2 heartbeat.",
    # Group Features - Scanning Detection  
    "Scan Group Unique Dst Port Count": "[H√†nh vi ƒë√°ng ng·ªù Scanning] **S·ªë port ƒë√≠ch unique trong group scanning**, s·ªë l∆∞·ª£ng l·ªõn ch·ªâ ra port scanning ho·∫∑c service discovery attacks.",
    "Scan Group Rej Ratio": "[H√†nh vi ƒë√°ng ng·ªù Scanning] **T·ª∑ l·ªá k·∫øt n·ªëi b·ªã reject trong group**, t·ª∑ l·ªá cao ch·ªâ ra reconnaissance scanning ho·∫∑c brute force attempts.",
    # Group Features - DDoS Detection
    "Ddos Group Unique Src Ip Count": "[H√†nh vi ƒë√°ng ng·ªù DDoS] **S·ªë IP ngu·ªìn unique trong group**, s·ªë l∆∞·ª£ng l·ªõn c√≥ th·ªÉ ch·ªâ ra distributed attack ho·∫∑c botnet activity.",
    "Ddos Group Total Bytes": "[H√†nh vi ƒë√°ng ng·ªù DDoS] **T·ªïng bytes trong group DDoS**, l∆∞u l∆∞·ª£ng l·ªõn c√≥ th·ªÉ ch·ªâ ra volumetric attack ho·∫∑c data exfiltration.",
    
    # Missing mappings for important features
    "Is Auth Port": "[T·∫•n c√¥ng X√°c th·ª±c] K·∫øt n·ªëi ƒë·∫øn m·ªôt c·ªïng x√°c th·ª±c chu·∫©n (21, 22, 23), tƒÉng m·ª©c ƒë·ªô nghi ng·ªù khi c√≥ l·ªói.",
    "Hist R Count": "[H√†nh vi ƒë√°ng ng·ªù] **S·ªë l∆∞·ª£ng g√≥i tin b·ªã reset trong l·ªãch s·ª≠ k·∫øt n·ªëi**, c√≥ th·ªÉ ch·ªâ ra scanning behavior ho·∫∑c connection probing.",
    "Is Auth Service": "[T·∫•n c√¥ng X√°c th·ª±c] H√†nh vi nh·∫Øm v√†o m·ªôt d·ªãch v·ª• x√°c th·ª±c (SSH, FTP...), m·ªôt m·ª•c ti√™u ph·ªï bi·∫øn c·ªßa t·∫•n c√¥ng brute-force.",
    "Failed Connection Ratio": "[H√†nh vi b·∫•t th∆∞·ªùng] **T·ª∑ l·ªá k·∫øt n·ªëi th·∫•t b·∫°i** l√† m·ªôt ƒë·∫∑c ƒëi·ªÉm m·∫°nh m·∫Ω kh√¥ng th∆∞·ªùng th·∫•y trong traffic b√¨nh th∆∞·ªùng.",
    
    # DNS Log Features - B·ªï sung th√™m cho Autoencoder
    "Query Length": "[Reconstruction Error - Query Length] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o ƒë·ªô d√†i query DNS** do gi√° tr·ªã b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data exfiltration, ho·∫∑c malicious payload.",
    "Query Entropy": "[Reconstruction Error - Query Entropy] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o entropy query DNS** do gi√° tr·ªã b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c obfuscated communication.",
    "Subdomain Count": "[Reconstruction Error - Subdomain Count] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o s·ªë subdomain** do gi√° tr·ªã b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data encoding, ho·∫∑c malicious domain generation.",
    "Numeric Ratio": "[Reconstruction Error - Numeric Ratio] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o t·ª∑ l·ªá s·ªë trong query DNS** do gi√° tr·ªã b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c binary data encoding.",
    "Ngram Score": "[Reconstruction Error - Ngram Score] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o ƒëi·ªÉm n-gram** do gi√° tr·ªã b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra encoded data, obfuscated communication, ho·∫∑c malicious payload.",
    "Has Base64 Pattern": "[Reconstruction Error - Has Base64 Pattern] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o pattern Base64** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra data encoding, DNS tunneling, ho·∫∑c malicious payload.",
    "Has Hex Pattern": "[Reconstruction Error - Has Hex Pattern] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o pattern Hex** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c binary data transmission.",
    "Has Long Subdomain": "[Reconstruction Error - Has Long Subdomain] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o subdomain d√†i** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data encoding, ho·∫∑c malicious domain generation.",
    "Suspicious Length": "[Reconstruction Error - Suspicious Length] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o ƒë·ªô d√†i ƒë√°ng ng·ªù** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data exfiltration, ho·∫∑c malicious communication.",
    "Char Diversity": "[Reconstruction Error - Char Diversity] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o ƒëa d·∫°ng k√Ω t·ª±** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c obfuscated communication.",
    "Vowel Consonant Ratio": "[Reconstruction Error - Vowel Consonant Ratio] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o t·ª∑ l·ªá nguy√™n √¢m/ph·ª• √¢m** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra encoded data, DNS tunneling, ho·∫∑c non-human generated queries.",
    "Compressed Pattern": "[Reconstruction Error - Compressed Pattern] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o pattern n√©n** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra compressed data, DNS tunneling, ho·∫∑c obfuscated communication.",
    "Unusual TLD": "[Reconstruction Error - Unusual TLD] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o TLD b·∫•t th∆∞·ªùng** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra malicious domain, DNS tunneling, ho·∫∑c command & control communication.",
    "Avg TTL": "[Reconstruction Error - Avg TTL] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o TTL trung b√¨nh** do gi√° tr·ªã b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra DNS tunneling, malicious DNS, ho·∫∑c command & control infrastructure.",
    "Min TTL": "[Reconstruction Error - Min TTL] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o TTL t·ªëi thi·ªÉu** do gi√° tr·ªã b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra DNS tunneling, fast-flux DNS, ho·∫∑c malicious infrastructure.",
    "Is Qtype TXT": "[Reconstruction Error - Is Qtype TXT] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o query TXT** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data exfiltration, ho·∫∑c command & control communication.",
    "Is Qtype NULL": "[Reconstruction Error - Is Qtype NULL] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o query NULL** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra DNS tunneling, data exfiltration, ho·∫∑c malicious DNS usage.",
    "Is NXDOMAIN": "[Reconstruction Error - Is NXDOMAIN] **Model g·∫∑p kh√≥ khƒÉn t√°i t·∫°o response NXDOMAIN** do t√≠nh ch·∫•t b·∫•t th∆∞·ªùng - c√≥ th·ªÉ ch·ªâ ra DNS tunneling, malicious domain queries, ho·∫∑c reconnaissance."
}

def translate_shap_to_human_readable(shap_explanation_list: list, connection_details: dict = None, log_type: str = 'conn') -> list:
    """
    Translate SHAP explanations to human-readable format for both conn.log and dns.log.
    
    Args:
        shap_explanation_list: List of SHAP explanations
        connection_details: Connection or DNS details
        log_type: 'conn' for connection logs, 'dns' for DNS logs
        
    Returns:
        List of human-readable explanations
    """
    try:
        human_readable = []
        
        for explanation in shap_explanation_list:
            feature = explanation.get('feature', 'Unknown')
            value = explanation.get('value', 0)
            direction = explanation.get('direction', 'normal')
            
            # ‚ö° OPTIMIZED: Create better human-readable explanation based on log type
            if log_type == 'dns':
                if direction == 'tunneling':
                    if value < -0.1:  # Strong negative SHAP value
                        human_explanation = f"üö® **C·∫¢NH B√ÅO CAO**: Ph√°t hi·ªán m·∫°nh m·∫Ω DNS tunneling qua ƒë·∫∑c tr∆∞ng '{feature}'. ƒê√¢y c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa vi·ªác tu·ªìn d·ªØ li·ªáu b·∫•t h·ª£p ph√°p ho·∫∑c command & control communication."
                    else:
                        human_explanation = f"‚ö†Ô∏è **C·∫¢NH B√ÅO TRUNG B√åNH**: Ph√°t hi·ªán v·ª´a ph·∫£i DNS tunneling qua ƒë·∫∑c tr∆∞ng '{feature}'. C·∫ßn theo d√µi th√™m ƒë·ªÉ x√°c ƒë·ªãnh m·ª©c ƒë·ªô nguy hi·ªÉm."
                else:
                    if value > 0.1:  # Strong positive SHAP value
                        human_explanation = f" **B√åNH TH∆Ø·ªúNG**: ƒê·∫∑c tr∆∞ng '{feature}' cho th·∫•y traffic DNS ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng, kh√¥ng c√≥ d·∫•u hi·ªáu b·∫•t th∆∞·ªùng."
                    else:
                        human_explanation = f"‚ÑπÔ∏è **B√åNH TH∆Ø·ªúNG**: ƒê·∫∑c tr∆∞ng '{feature}' cho th·∫•y traffic DNS ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng."
            else:  # conn.log
                if direction == 'anomaly':
                    if value < -0.1:  # Strong negative SHAP value
                        human_explanation = f"üö® **C·∫¢NH B√ÅO CAO**: Ph√°t hi·ªán m·∫°nh m·∫Ω network anomaly qua ƒë·∫∑c tr∆∞ng '{feature}'. ƒê√¢y c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa port scanning, DDoS attack, ho·∫∑c C2 beaconing."
                    else:
                        human_explanation = f"‚ö†Ô∏è **C·∫¢NH B√ÅO TRUNG B√åNH**: Ph√°t hi·ªán v·ª´a ph·∫£i network anomaly qua ƒë·∫∑c tr∆∞ng '{feature}'. C·∫ßn theo d√µi th√™m ƒë·ªÉ x√°c ƒë·ªãnh m·ª©c ƒë·ªô nguy hi·ªÉm."
                else:
                    if value > 0.1:  # Strong positive SHAP value
                        human_explanation = f" **B√åNH TH∆Ø·ªúNG**: ƒê·∫∑c tr∆∞ng '{feature}' cho th·∫•y network traffic ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng, kh√¥ng c√≥ d·∫•u hi·ªáu b·∫•t th∆∞·ªùng."
                    else:
                        human_explanation = f"‚ÑπÔ∏è **B√åNH TH∆Ø·ªúNG**: ƒê·∫∑c tr∆∞ng '{feature}' cho th·∫•y network traffic ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng."
            
            human_readable.append({
                "feature": feature,
                "explanation": human_explanation,
                "impact": abs(value),
                "direction": direction,
                "log_type": log_type
            })
        
        return human_readable
        
    except Exception as e:
        logger.warning(f"Error translating SHAP to human readable for {log_type}: {str(e)}")
        return []

def translate_ae_to_human_readable(ae_explanation_list: list) -> list:
    """
    Translates a technical Autoencoder explanation list into human-readable security insights.
    
    Args:
        ae_explanation_list: List of Autoencoder explanation dictionaries with 'feature' and 'contribution_percent' keys
        
    Returns:
        List of human-readable security narratives explaining why the model had difficulty reconstructing features
    """
    if not ae_explanation_list:
        return []
    
    # Process autoencoder explanation for human-readable narratives
    
    narratives = []
    for item in ae_explanation_list:
        feature_name = item.get('feature', '')
        
        # The feature_name from AE explanation is already cleaned by format_ae_explanation
        # So we try direct matching first, then fallback to cleaning if needed
        logger.info(f"AE Processing feature: {feature_name}")
        
        # Find the best matching key in the Autoencoder interpretation dictionary
        matched_key = None
        
        # ‚ö° OPTIMIZED: Try exact match first with the already-cleaned feature name
        if feature_name in HUMAN_INTERPRETATIONS_AE:
            matched_key = feature_name
            logger.info(f"AE EXACT MATCH: {feature_name} -> {matched_key}")
        else:
            # ‚ö° OPTIMIZED: Try partial matching with the feature name as-is
            for key in HUMAN_INTERPRETATIONS_AE:
                if (key.lower() in feature_name.lower() or 
                    feature_name.lower() in key.lower()):
                    matched_key = key
                    logger.info(f"AE PARTIAL MATCH: {feature_name} -> {key}")
                    break
            
            # ‚ö° OPTIMIZED: If still no match, try cleaning the feature name (fallback)
            if not matched_key:
                clean_feature = clean_feature_name(feature_name)
                logger.info(f"AE Trying cleaned version: {feature_name} -> {clean_feature}")
                
                if clean_feature in HUMAN_INTERPRETATIONS_AE:
                    matched_key = clean_feature
                    logger.info(f"AE CLEANED MATCH: {feature_name} -> {clean_feature} -> {matched_key}")
                else:
                    for key in HUMAN_INTERPRETATIONS_AE:
                        if (key.lower() in clean_feature.lower() or 
                            clean_feature.lower() in key.lower()):
                            matched_key = key
                            logger.info(f"AE CLEANED PARTIAL MATCH: {feature_name} -> {clean_feature} -> {key}")
                            break
        
        if matched_key:
            narratives.append(HUMAN_INTERPRETATIONS_AE[matched_key])
            logger.info(f"AE XAI: ‚úì Added interpretation for '{feature_name}' -> '{matched_key}'")
        else:
            logger.info(f"AE XAI: ‚úó No interpretation found for feature '{feature_name}'")
    
    # Return unique narratives to avoid duplicates
    unique_narratives = list(dict.fromkeys(narratives))
    logger.info(f"AE XAI: Final result - {len(unique_narratives)} unique explanations from {len(ae_explanation_list)} features")
    logger.info(f"üîç AE XAI OUTPUT: {unique_narratives}")
    
    return unique_narratives

def get_autoencoder_explanation(X_original: np.ndarray, X_reconstructed: np.ndarray, 
                               feature_names: Optional[List[str]] = None, top_n: int = 5) -> List[Dict[str, Any]]:
    """
    Generate native explainability for Autoencoder based on per-feature reconstruction error.
    
    Args:
        X_original: Original preprocessed data (1D array)
        X_reconstructed: Reconstructed data from autoencoder (1D array)
        feature_names: Names of the features (optional)
        top_n: Number of top contributing features to return
        
    Returns:
        List of dictionaries with feature names and reconstruction errors
    """
    try:
        if feature_names is None or len(feature_names) != len(X_original):
            # Fallback feature names if not available
            feature_names = [f"feature_{i}" for i in range(len(X_original))]
        
        # Calculate squared error for each feature
        feature_errors = np.power(X_original - X_reconstructed, 2).flatten()
        
        # Create DataFrame for easy sorting
        error_df = pd.DataFrame({
            'feature': feature_names,
            'error': feature_errors,
            'original_value': X_original.flatten(),
            'reconstructed_value': X_reconstructed.flatten(),
            'difference': np.abs(X_original - X_reconstructed).flatten()
        })
        
        # Sort by error in descending order
        error_df = error_df.sort_values('error', ascending=False)
        
        # Get top N features
        top_features = error_df.head(top_n)
        
        # Convert to list of dictionaries for easy use in UI
        explanation_list = []
        for _, row in top_features.iterrows():
            explanation_list.append({
                'feature': clean_feature_name(row['feature']),
                'error': row['error'],
                'original_value': row['original_value'],
                'reconstructed_value': row['reconstructed_value'],
                'difference': row['difference'],
                'contribution_percent': (row['error'] / feature_errors.sum()) * 100 if feature_errors.sum() > 0 else 0
            })
        
        return explanation_list
        
    except Exception as e:
        logger.warning(f"Error generating Autoencoder explanation: {str(e)}")
        return []

def get_shap_explanation(shap_values: np.ndarray, feature_names: Optional[List[str]] = None, 
                        top_n: int = 5) -> Dict[str, Any]:
    """
    Generate human-readable SHAP explanation for anomaly detection.
    
    Args:
        shap_values: SHAP values for the prediction
        feature_names: Names of the features
        top_n: Number of top contributing features to return
        
    Returns:
        Dictionary with explanation data
    """
    try:
        if shap_values is None:
            return {'error': 'SHAP values not available'}
        
        if feature_names is None:
            # Fallback feature names if not available
            feature_names = [f"feature_{i}" for i in range(len(shap_values))]
        
        # Ensure we have the right number of feature names
        n_features = len(shap_values)
        if len(feature_names) < n_features:
            # Pad with generic names if needed
            feature_names = feature_names + [f'feature_{i}' for i in range(len(feature_names), n_features)]
        else:
            feature_names = feature_names[:n_features]
        
        # Create DataFrame with features and their SHAP values
        explanation_df = pd.DataFrame({
            'feature': feature_names,
            'shap_value': shap_values,
            'abs_shap_value': np.abs(shap_values)
        })
        
        # Sort by absolute SHAP value (importance)
        explanation_df = explanation_df.sort_values('abs_shap_value', ascending=False)
        
        # Get top contributing features
        top_features = explanation_df.head(top_n)
        
        # Calculate total influence
        total_influence = np.sum(np.abs(shap_values))
        
        explanation = {
            'top_features': top_features.to_dict('records'),
            'total_influence': total_influence,
            'explanation_summary': generate_explanation_text(top_features)
        }
        
        return explanation
        
    except Exception as e:
        return {'error': f'Error generating explanation: {str(e)}'}

def format_shap_explanation(shap_values: np.ndarray, feature_names: Optional[List[str]] = None, 
                           top_n: int = 5, log_type: str = 'conn') -> Dict[str, Any]:
    """
    Format SHAP values into standardized explanation format for both conn.log and dns.log.
    
    Args:
        shap_values: SHAP values array
        feature_names: List of feature names
        top_n: Number of top features to return
        log_type: 'conn' for connection logs, 'dns' for DNS logs
        
    Returns:
        Dict with structured SHAP explanation including top_features, summary, and total_influence
    """
    try:
        # Process SHAP values for explanation
        
        if feature_names is None or len(feature_names) < len(shap_values):
            # Fallback feature names
            logger.warning(f"üö® SHAP falling back to generic names! feature_names={len(feature_names) if feature_names else 0}, shap_values={len(shap_values)}")
            feature_names = [f"{log_type.upper()}_Feature {i}" for i in range(len(shap_values))]
        else:
            feature_names = feature_names[:len(shap_values)]
            logger.debug(f"Using meaningful feature names for {log_type} SHAP explanation")
        
        # Create explanation list
        explanations = []
        feature_importance = list(zip(feature_names, shap_values, np.abs(shap_values)))
        
        # Sort by absolute importance
        feature_importance.sort(key=lambda x: x[2], reverse=True)
        
        for feature_name, shap_value, abs_value in feature_importance[:top_n]:
            # Use appropriate direction based on log type
            if log_type == 'dns':
                direction = "tunneling" if shap_value < 0 else "normal"
            else:  # conn.log
                direction = "anomaly" if shap_value < 0 else "normal"
            
            explanations.append({
                "feature": clean_feature_name(feature_name),
                "shap_value": float(shap_value),
                "importance": float(abs_value),
                "direction": direction,
                "log_type": log_type  # Add log type for identification
            })
        
        # Calculate total influence
        total_influence = np.sum(np.abs(shap_values))
        
        # Generate summary text
        summary = generate_shap_summary(explanations, log_type)
        
        # Return structured dict format that dashboard expects
        return {
            'top_features': explanations,
            'total_influence': float(total_influence),
            'summary': summary,
            'log_type': log_type,
            'feature_count': len(explanations)
        }
        
    except Exception as e:
        logger.warning(f"Error formatting SHAP explanation for {log_type}: {str(e)}")
        return {
            'top_features': [],
            'total_influence': 0.0,
            'summary': f"Error: {str(e)}",
            'log_type': log_type,
            'feature_count': 0
        }

def format_ae_explanation(X_original: np.ndarray, X_reconstructed: np.ndarray, 
                         feature_names: Optional[List[str]] = None, top_n: int = 5) -> List[Dict[str, Any]]:
    """
    Format Autoencoder explanation into standardized format.
    
    Args:
        X_original: Original preprocessed data
        X_reconstructed: Reconstructed data from autoencoder
        feature_names: List of feature names
        top_n: Number of top features to return
        
    Returns:
        List of feature explanations in standard format
    """
    try:
        # Process autoencoder data for explanation
        
        # ‚ö° DEBUG: Log feature_names tr∆∞·ªõc khi x·ª≠ l√Ω
        logger.info(f"üîç AE FORMAT DEBUG: Received feature_names count={len(feature_names) if feature_names else 0}")
        logger.info(f"üîç AE FORMAT DEBUG: X_original shape={X_original.shape}, size={X_original.size}")
        
        if feature_names is None or len(feature_names) < X_original.size:
            logger.warning(f"üö® AE falling back to generic names! feature_names={len(feature_names) if feature_names else 0}, X_original size={X_original.size}")
            feature_names = [f"Feature {i}" for i in range(X_original.size)]
        else:
            feature_names = feature_names[:X_original.size]
            logger.debug(f"Using meaningful feature names for AE explanation")
        
        # Calculate per-feature reconstruction errors
        feature_errors = np.power(X_original - X_reconstructed, 2).flatten()
        
        # ‚ö° DEBUG: Log feature counts ƒë·ªÉ ƒë·∫£m b·∫£o 5 features
        logger.info(f"üîç AE FORMAT DEBUG: X_original shape={X_original.shape}, X_reconstructed shape={X_reconstructed.shape}")
        logger.info(f"üîç AE FORMAT DEBUG: feature_errors count={len(feature_errors)}, feature_names count={len(feature_names)}")
        logger.info(f"üîç AE FORMAT DEBUG: top_n={top_n}, will return {min(top_n, len(feature_errors))} features")
        
        # Create explanation list
        explanations = []
        feature_data = list(zip(feature_names, feature_errors, X_original.flatten(), X_reconstructed.flatten()))
        
        # Sort by error (descending)
        feature_data.sort(key=lambda x: x[1], reverse=True)
        
        total_error = np.sum(feature_errors)
        
        for feature_name, error, orig_val, recon_val in feature_data[:top_n]:
            explanations.append({
                "feature": clean_feature_name(feature_name),
                "error": float(error),
                "original_value": float(orig_val),
                "reconstructed_value": float(recon_val),
                "difference": float(abs(orig_val - recon_val)),
                "contribution_percent": float((error / total_error) * 100) if total_error > 0 else 0.0
            })
        
        return explanations
        
    except Exception as e:
        logger.warning(f"Error formatting Autoencoder explanation: {str(e)}")
        return []

def generate_explanation_text(top_features: pd.DataFrame) -> str:
    """Generate human-readable explanation text."""
    try:
        explanations = []
        
        for _, row in top_features.iterrows():
            feature = row['feature']
            shap_value = row['shap_value']
            
            # Determine if feature pushes toward anomaly or normal
            direction = "üî¥ toward ANOMALY" if shap_value < 0 else "üü¢ toward NORMAL"  # Correct logic for IsolationForest
            
            # Clean up feature names for display
            clean_feature = clean_feature_name(feature)
            
            explanation = f"**{clean_feature}**: {direction} (impact: {abs(shap_value):.3f})"
            explanations.append(explanation)
        
        return "\n\n".join(explanations)
        
    except Exception:
        return "Could not generate explanation text"

def generate_shap_summary(explanations: List[Dict[str, Any]], log_type: str = 'conn') -> str:
    """Generate human-readable summary of SHAP explanations."""
    try:
        if not explanations:
            return "No significant features identified"
        
        # Count anomaly vs normal indicators
        anomaly_count = sum(1 for exp in explanations if exp.get('direction') == 'anomaly')
        normal_count = len(explanations) - anomaly_count
        
        # Get top feature
        top_feature = explanations[0] if explanations else {}
        top_feature_name = top_feature.get('feature', 'Unknown')
        top_importance = top_feature.get('importance', 0.0)
        
        if log_type == 'dns':
            summary = f"DNS query analysis shows {len(explanations)} significant features. "
            if anomaly_count > normal_count:
                summary += f"Top indicator: {top_feature_name} (tunneling score: {top_importance:.3f})"
            else:
                summary += f"Top indicator: {top_feature_name} (normal score: {top_importance:.3f})"
        else:  # conn.log
            summary = f"Connection analysis shows {len(explanations)} significant features. "
            if anomaly_count > normal_count:
                summary += f"Top anomaly indicator: {top_feature_name} (impact: {top_importance:.3f})"
            else:
                summary += f"Top normal indicator: {top_feature_name} (impact: {top_importance:.3f})"
        
        return summary
        
    except Exception as e:
        logger.warning(f"Error generating SHAP summary: {str(e)}")
        return "Summary generation failed"

def clean_feature_name(feature_name: str) -> str:
    """Clean up feature names for better readability with enhanced features support."""
    # Enhanced mapping for all features including new enhanced ones
    name_mapping = {
        # Original features
        'duration': 'Connection Duration',
        'orig_bytes': 'Bytes Sent',
        'resp_bytes': 'Bytes Received',
        'orig_pkts': 'Packets Sent',
        'resp_pkts': 'Packets Received',
        'orig_ip_bytes': 'Orig Ip Bytes',
        'resp_ip_bytes': 'Resp Ip Bytes',
        'hist_len': 'History Length',
        'hist_R_count': 'Reset Flags',
        'hist_has_T': 'Timeout Flag',
        'missed_bytes': 'Missed Bytes',
        # Enhanced network behavior features
        'bytes_ratio': 'Bytes Ratio',
        'packets_ratio': 'Packets Ratio',
        'avg_packet_size_orig': 'Avg Packet Size Orig',
        'avg_packet_size_resp': 'Avg Packet Size Resp',
        'connection_rate': 'Connection Rate',
        'is_failed_connection': 'Failed Connection Ratio',
        # Enhanced categorical features
        'duration_category': 'Duration Category',
        'traffic_pattern': 'Traffic Pattern',
        'orig_port_binned': 'Orig Port Binned',
        'resp_port_binned': 'Resp Port Binned',
        'service_binned': 'Service Binned',
        'proto': 'Protocol',
        'conn_state': 'Connection State',
        # NEW: C2 Beaconing specific features
        'small_consistent_size': 'Small Consistent Size',
        'heartbeat_candidate': 'Heartbeat Candidate',
        'periodic_score': 'Periodic Score',
        'beaconing_pattern': 'Beaconing Pattern'
    }
    
    # Clean categorical feature names (remove sklearn prefixes)
    if feature_name.startswith('cat__'):
        feature_name = feature_name.replace('cat__', '')
    
    # Handle specific categorical feature patterns
    if 'traffic_pattern_' in feature_name.lower():
        pattern_type = feature_name.lower().replace('traffic_pattern_', '').replace('cat__', '')
        return f'Traffic Pattern {pattern_type.replace("_", " ").title()}'
    
    if 'duration_category_' in feature_name.lower():
        duration_type = feature_name.lower().replace('duration_category_', '').replace('cat__', '')
        return f'Duration Category {duration_type.replace("_", " ").title()}'
    
    if 'orig_port_binned_' in feature_name.lower():
        port_type = feature_name.lower().replace('orig_port_binned_', '').replace('cat__', '')
        return f'Orig Port Binned {port_type.replace("_", " ").title()}'
    
    if 'resp_port_binned_' in feature_name.lower():
        port_type = feature_name.lower().replace('resp_port_binned_', '').replace('cat__', '')
        return f'Resp Port Binned {port_type.replace("_", " ").title()}'
    
    if 'service_binned_' in feature_name.lower():
        service_type = feature_name.lower().replace('service_binned_', '').replace('cat__', '')
        return f'Service Binned {service_type.replace("_", " ").title()}'
    
    if 'conn_state_' in feature_name.lower():
        state_type = feature_name.lower().replace('conn_state_', '').replace('cat__', '')
        return f'Conn State {state_type.upper()}'
    
    if 'proto_' in feature_name.lower():
        proto_type = feature_name.lower().replace('proto_', '').replace('cat__', '')
        return f'Proto {proto_type.upper()}'
    
    # Handle NEW C2 beaconing pattern features
    if 'beaconing_pattern_' in feature_name.lower():
        pattern_type = feature_name.lower().replace('beaconing_pattern_', '').replace('cat__', '')
        return f'Beaconing Pattern {pattern_type.replace("_", " ").title()}'
    
    # Handle group features (beacon_group_count, etc.)
    if 'beacon_group_count' in feature_name.lower():
        return 'Beacon Group Count'
    if 'beacon_group_mean_interval' in feature_name.lower():
        return 'Beacon Group Mean Interval'
    if 'beacon_group_cv' in feature_name.lower():
        return 'Beacon Group Cv'
    if 'scan_group_unique_dst_port_count' in feature_name.lower():
        return 'Scan Group Unique Dst Port Count'
    if 'scan_group_rej_ratio' in feature_name.lower():
        return 'Scan Group Rej Ratio'
    if 'ddos_group_unique_src_ip_count' in feature_name.lower():
        return 'Ddos Group Unique Src Ip Count'
    if 'ddos_group_total_bytes' in feature_name.lower():
        return 'Ddos Group Total Bytes'
    
    # Handle special features that might be missing
    if 'is_auth_port' in feature_name.lower():
        return 'Is Auth Port'
    if 'hist_r_count' in feature_name.lower():
        return 'Hist R Count'

    if 'is_failed_connection' in feature_name.lower():
        return 'Failed Connection Ratio'
    
    # Apply direct mapping if available
    feature_lower = feature_name.lower()
    for key, value in name_mapping.items():
        if key in feature_lower:
            return value
    
    # Clean up remaining names (fallback)
    clean_name = feature_name.replace('_', ' ').replace('cat__', '').title()
    return clean_name 

def clean_dns_feature_name(feature_name: str) -> str:
    """
    Clean DNS feature names for display.
    
    Args:
        feature_name: Raw feature name
        
    Returns:
        Cleaned feature name for display
    """
    try:
        # Remove common prefixes
        if feature_name.startswith('dns_'):
            feature_name = feature_name[4:]
        
        # Replace underscores with spaces
        feature_name = feature_name.replace('_', ' ')
        
        # Capitalize first letter of each word
        feature_name = ' '.join(word.capitalize() for word in feature_name.split())
        
        # Special DNS feature name mappings
        dns_mappings = {
            'query length': 'Query Length',
            'response length': 'Response Length',
            'query entropy': 'Query Entropy',
            'response entropy': 'Response Entropy',
            'subdomain count': 'Subdomain Count',
            'domain length': 'Domain Length',
            'query type': 'Query Type',
            'response type': 'Response Type',
            'ttl': 'TTL',
            'rcode': 'Response Code'
        }
        
        return dns_mappings.get(feature_name.lower(), feature_name)
        
    except Exception as e:
        logger.warning(f"Error cleaning DNS feature name '{feature_name}': {str(e)}")
        return feature_name 